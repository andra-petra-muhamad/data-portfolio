{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khsuxDLIoIrs"
   },
   "source": [
    "# Machine Translation with Transformer\n",
    "Terjemahan mesin (machine translation) merupakan salah satu tonggak penting dalam perkembangan Natural Language Processing (NLP). Model Transformer, yang diperkenalkan oleh Vaswani dkk. (2017), menjadi revolusi besar karena mampu menggantikan arsitektur berbasis RNN dan CNN dengan mekanisme attention yang lebih efisien.\n",
    "\n",
    "Dalam notebook ini, akan dibangun sebuah model Transformer dari nol untuk melakukan penerjemahan otomatis dari bahasa Inggris ke bahasa Spanyol. Pendekatan ini tidak menggunakan model pra-latih, melainkan mengimplementasikan komponen inti Transformer, mulai dari positional encoding, multi-head attention, encoder-decoder layer, hingga proses training.\n",
    "\n",
    "Fokus utama bukan hanya pada hasil terjemahan, melainkan juga pada kemampuan implementasi dan pemahaman mekanisme internal Transformer. Hal ini menjadi relevan karena Transformer adalah fondasi dari berbagai model modern seperti BERT, GPT, dan ChatGPT, sehingga menguasai arsitektur dasarnya menjadi bekal penting bagi seorang Machine Learning Engineer maupun Data Scientist.\n",
    "\n",
    "The dataset source is www.manythings.org/anki\n",
    "\n",
    "## Sequence-to-sequence learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ98ptbtoRii"
   },
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3cJAoIeTXqIx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Bidirectional,GRU,LSTM,Embedding\n",
    "from tensorflow.keras.layers import Dense,MultiHeadAttention,LayerNormalization,Embedding,Dropout,Layer\n",
    "from tensorflow.keras import Sequential,Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps4nkpcHoW7D"
   },
   "source": [
    "### Download dan Ekstrak Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download ZIP (tanpa ekstrak otomatis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = keras.utils.get_file(\n",
    "    \"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ekstraksi manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(pathlib.Path(path_to_zip).parent)\n",
    "\n",
    "text_file = pathlib.Path(path_to_zip).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melihat semua file hasil ekstraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIWHVxF-oZfa"
   },
   "source": [
    "### Persiapkan Pasangan Teks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR4qXBuAp6TN"
   },
   "source": [
    "Setiap baris berisi kalimat bahasa Inggris dan kalimat bahasa Spanyol yang sesuai. Kalimat bahasa Inggris adalah urutan sumber dan kalimat bahasa Spanyol adalah urutan target. Kami menambahkan token \"[start]\" di depan dan kami menambahkan token \"[end]\" di belakang kalimat bahasa Spanyol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8XXQjzxlXxBq"
   },
   "outputs": [],
   "source": [
    "# Start dan end menandakan mulai diubah dan berhentinya translate kata/frasa tersebut\n",
    "\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNV4qYbApu7K",
    "outputId": "45f01629-6967-4414-f0c9-b1356daaf5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs: 118964\n",
      "Example text: (\"I'll be fine.\", '[start] Estaré bien. [end]')\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pairs:\", len(text_pairs))\n",
    "print(\"Example text:\", text_pairs[2900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McidSg9oqA5e"
   },
   "source": [
    "Printing pasangan teks acak Inggris - Spanyol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dau5SsHEqDt2",
    "outputId": "b03c8b10-fffe-46c8-e942-c54411c4875c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Where can I buy envelopes?', '[start] ¿Dónde puedo comprar sobres? [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUOejgj9ob6C"
   },
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOtPnpe-qL-0"
   },
   "source": [
    "- train_pairs: 70% awal data\n",
    "- val_pairs: 15% berikutnya\n",
    "- test_pairs: 15% terakhir\n",
    "- 70% training: cukup besar untuk belajar pola.\n",
    "- 15% validation: cukup untuk tuning hyperparameter.\n",
    "- 15% test: cukup untuk mengukur performa generalisasi model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Menentukan jumlah data validasi 15% dari total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "\n",
    "num_val_samples = int(0.15 * len(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mengambil sisa data untuk training, setelah menyisihkan 15% untuk validasi dan 15% untuk testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8aMQvgqjXzre"
   },
   "outputs": [],
   "source": [
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymKGiQPLqOF1",
    "outputId": "b2a6b9a3-0e88-4d90-8583-20c3a3d3a2a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 83276\n",
      "Number of validation data: 17844\n",
      "Number of test data: 17844\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train data:\", len(train_pairs))\n",
    "print(\"Number of validation data:\", len(val_pairs))\n",
    "print(\"Number of test data:\", len(test_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8wh2_Ahoe-S"
   },
   "source": [
    "### Preprocessing dan Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Otldv3LkqiUO"
   },
   "source": [
    "* string.punctuation adalah built-in Python yang berisi semua karakter tanda baca umum\n",
    "* Ditambah + \"¿\" karena simbol “¿” (inverted question mark) adalah tanda tanya pembuka dalam bahasa Spanyol (contoh: ¿Cómo estás?).\n",
    "* Membuat daftar semua karakter yang dianggap \"tanda baca yang ingin dihapus atau dibersihkan dari teks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0-7VqsMGqkrM"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(string.punctuation)}¿]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "doKPFLU8qnHy"
   },
   "outputs": [],
   "source": [
    "vocab_size = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artinya: hanya 15.000 kata yang paling sering muncul yang akan digunakan dalam kamus (vocab). Selanjutnya, menentukan panjang maksimal urutan token setelah teks di-tokenisasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWkolLapqxD-"
   },
   "source": [
    "* Membuat vektorisasi untuk input (bahasa Inggris)\n",
    "* Digunakan untuk encoder, yang hanya perlu urutan teks biasa (tanpa token khusus seperti [start] / [end])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tWaJKgRwqyc0"
   },
   "outputs": [],
   "source": [
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88rmdmeqq1M1"
   },
   "source": [
    "* Membuat vektorisasi untuk target (bahasa Spanyol)\n",
    "* output_sequence_length=20: Karena target punya token [start] dan [end], maka panjang target lebih panjang 1 token dari input\n",
    "* standardize=custom_standardization: Menggunakan fungsi pembersih teks kustom (misal: hilangkan tanda baca tertentu tapi pertahankan [start] dan [end])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_jiWvg3SX2DK"
   },
   "outputs": [],
   "source": [
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ekstrak semua kalimat bahasa Inggris dari pasangan data pelatihan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_texts = [pair[0] for pair in train_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ekstrak semua kalimat bahasa Spanyol (dengan [start] dan [end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spanish_texts = [pair[1] for pair in train_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melatih TextVectorization layer untuk membangun vocabulary berdasarkan teks pelatihan.\n",
    "`.adapt()` penting agar vektorisasi mengenali kata-kata umum dan menyusun vocab secara efisien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FfrvQO_cq4ud"
   },
   "outputs": [],
   "source": [
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8Ow3sX6oiMk"
   },
   "source": [
    "### Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artinya: Model akan memproses 64 pasangan kalimat (Inggris–Spanyol) dalam satu langkah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eSuhef1WX4FO"
   },
   "outputs": [],
   "source": [
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng) # Tokenisasi input (bahasa Inggris)\n",
    "    spa = target_vectorization(spa) # Tokenisasi target (bahasa Spanyol)\n",
    "    return ({\"english\": eng, \"spanish\": spa[:, :-1]}, spa[:, 1:])\n",
    "    # Semua token target KECUALI yang terakhir atau KECUALI pertama\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    # Pisahkan pasangan (eng, spa) jadi 2 list\n",
    "    eng_texts, spa_texts = zip(*pairs) \n",
    "    \n",
    "    # Buat tf.data.Dataset dari pasangan teks\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(spa_texts)))\n",
    "    dataset = dataset.batch(batch_size) # Bagi data jadi batch ukuran 64\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE) # Format isi batch dengan format_dataset\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache() # Simpan data hasil transformasi di memori untuk efisiensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9EXsSyv-rgWa",
    "outputId": "f2809091-2efd-44cd-8e59-ce44f0f17e2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CacheDataset element_spec=({'english': TensorSpec(shape=(None, 20), dtype=tf.int64, name=None), 'spanish': TensorSpec(shape=(None, 20), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 20), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ery4gUVhrhut",
    "outputId": "3f44baeb-991d-417d-8d36-863a23fd7b07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CacheDataset element_spec=({'english': TensorSpec(shape=(None, 20), dtype=tf.int64, name=None), 'spanish': TensorSpec(shape=(None, 20), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 20), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yx12crftrkjQ",
    "outputId": "876ea7b8-614a-409d-d927-c02188d01c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english [[1439   13 1143 ...    0    0    0]\n",
      " [   3  214    7 ...    0    0    0]\n",
      " [  87   51    5 ...    0    0    0]\n",
      " ...\n",
      " [  15    5   17 ...    0    0    0]\n",
      " [   6  319  493 ...    0    0    0]\n",
      " [   6 1186    7 ...    0    0    0]],\n",
      "\n",
      "\n",
      " inputs['english'].shape: (64, 20)\n",
      "spanish [[   2   35  287 ...    0    0    0]\n",
      " [   2  434   18 ...    0    0    0]\n",
      " [   2   83  523 ...    0    0    0]\n",
      " ...\n",
      " [   2   94   18 ...    0    0    0]\n",
      " [   2    8 7995 ...    0    0    0]\n",
      " [   2    8   26 ...    0    0    0]],\n",
      "\n",
      "\n",
      " inputs['spanish'].shape: (64, 20)\n",
      "targets [[  35  287  113 ...    0    0    0]\n",
      " [ 434   18    1 ...    0    0    0]\n",
      " [  83  523   28 ...    0    0    0]\n",
      " ...\n",
      " [  94   18  531 ...    0    0    0]\n",
      " [   8 7995   84 ...    0    0    0]\n",
      " [   8   26 1367 ...    0    0    0]], \n",
      "\n",
      "\n",
      " targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"english {inputs['english']},\\n\\n\\n inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"spanish {inputs['spanish']},\\n\\n\\n inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets {targets}, \\n\\n\\n targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6BfulGQrnwE"
   },
   "source": [
    "## Sequence-to-sequence learning with Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBXzMh2xol1S"
   },
   "source": [
    "### Positional Embedding\n",
    "Layer ini menggabungkan embedding kata/token dengan embedding posisi sehingga model transformer dapat memahami urutan kata dalam sebuah kalimat. `PositionalEmbedding` ini memastikan bahwa transformer tidak hanya tahu kata apa yang ada, tetapi juga urutan kata dalam kalimat. Tanpa komponen ini, transformer hanya akan melihat kata sebagai “tas kata” (bag-of-words) tanpa urutan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kfr7tYlIYcio"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        #Menerjemahkan token ID (angka) menjadi vektor berdimensi output_dim.\n",
    "        self.token_embeddings = Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim, mask_zero=True  # aktifkan masking otomatis!\n",
    "        )\n",
    "        \n",
    "        #intermediate = self.getPositionEncoding(seq_len=input_dim,d=vocab_size,n=output_dim)\n",
    "        #Posisi 0, 1, 2, ..., sequence_length-1 diberi embedding seperti token biasa.\n",
    "        #Perbedaan: input ke sini adalah urutan posisi, bukan ID kata.\n",
    "        \n",
    "        self.position_embeddings = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Gunakan panjang tetap dari self.sequence_length\n",
    "        positions = tf.range(start=0, limit=tf.shape(inputs)[-1], delta=1)\n",
    "        positions = self.position_embeddings(positions) # (1, sequence_length)\n",
    "        x = self.token_embeddings(inputs)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"output_dim\": self.output_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIiAG8fWoxSS"
   },
   "source": [
    "### Transformer Encoder\n",
    "`TransformerEncoder` ini adalah blok encoder dari arsitektur transformer, yang berfungsi untuk menangkap hubungan antar kata dalam sebuah kalimat dengan menggunakan self-attention dan feed-forward network. \n",
    "\n",
    "`TransformerEncoder = Self-Attention + Feed-Forward + Residual + Normalization`\\\n",
    "Fungsinya adalah membangun representasi kata yang kontekstual → setiap kata tidak berdiri sendiri, tapi tahu hubungannya dengan kata lain dalam kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "hK4vdx9cX7Uj"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=None)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_MkR9QyozOk"
   },
   "source": [
    "### Transformer Decoder\n",
    "`TransformerDecoder` bertugas menghasilkan urutan keluaran (output sequence) berdasarkan representasi yang diberikan encoder, sambil memastikan proses generasi bersifat autoregressive (hanya melihat token sebelumnya). \n",
    "\n",
    "Singkatnya, decoder bertugas menggabungkan dua sisi:\n",
    "* Autoregressive generation (hanya melihat masa lalu lewat masked self-attention).\n",
    "* Konteks global dari input (melalui cross-attention ke encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hpLNJHi-X9pC"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    # Konstruktor yang berisi multihead ditambah masked self attention\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Masked self-attention: hanya melihat ke token sebelumnya, agar tidak \"mengintip masa depan\".\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        \n",
    "        #Cross-attention: menghubungkan output dari encoder ke decoder (input dari bahasa sumber).\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        \n",
    "        #Feedforward network: 2 layer Dense sebagai pemrosesan non-linear.\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        \n",
    "        #Normalisasi di setiap tahap (setelah residual connection).\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "    #Membuat mask segitiga bawah (causal mask) berukuran [batch, seq_len, seq_len]\n",
    "    #Mencegah posisi ke-i melihat ke token ke-j > i.\n",
    "    #Digunakan agar saat pelatihan model decoder tidak tahu token setelahnya (future blindness).\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        i = tf.range(tf.shape(inputs)[1])[:, tf.newaxis]\n",
    "        j = tf.range(tf.shape(inputs)[1])\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        return tf.reshape(mask, (1, tf.shape(inputs)[1], tf.shape(inputs)[1]))\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        \n",
    "        #Buat Mask untuk self-attention, agar tidak “mengintip ke depan”.\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        \n",
    "        #Masked Self-Attention\n",
    "        #Fokus hanya ke token sebelumnya atau saat ini. Tambah residual + layernorm\n",
    "        attention_output_1 = self.attention_1(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        \n",
    "        #Cross Attention (Decoder attends to Encoder)\n",
    "        #Menghubungkan representasi target dengan representasi yang\n",
    "        #dihasilkan oleh encoder (misalnya dari kalimat Inggris).\n",
    "        attention_output_2 = self.attention_2(attention_output_1, encoder_outputs, encoder_outputs)\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
    "        \n",
    "        #FFN memperkuat representasi token. Lalu dilakukan residual connection dan layer normalization.\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ29cZR3o3Ws"
   },
   "source": [
    "### Bangun Model Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Menentukan ukuran vektor representasi tiap token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Menentukan ukuran hidden layer dalam blok FFN di encoder dan decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_dim = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Menentukan ukuran banyak parallel attention heads dalam attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mendefinisikan input untuk bahasa sumber (English) dalam bentuk urutan token ID, dengan panjang dinamis (None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representasi token English yang mempertimbangkan urutan/posisi kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representasi konteks penuh untuk seluruh kalimat input (English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input berupa token ID untuk bahasa target (Spanish), digunakan saat training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "6cXSxHirX_lU"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ english             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spanish             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ english[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ spanish[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,155,456</span> │ positional_embed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,259,520</span> │ positional_embed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_deco… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,855,000</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ english             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spanish             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,845,120\u001b[0m │ english[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,845,120\u001b[0m │ spanish[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,155,456\u001b[0m │ positional_embed… │\n",
       "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m5,259,520\u001b[0m │ positional_embed… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ transformer_deco… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m3,855,000\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m15000\u001b[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretasi Model\n",
    "1. Input Layer\n",
    "    * english (InputLayer) → (None, None)\\\n",
    "      Input urutan teks bahasa Inggris (panjang urutan fleksibel).\n",
    "    * spanish (InputLayer) → (None, None)\\\n",
    "      Input urutan teks bahasa Spanyol (target sequence).\n",
    "2. Positional Embedding\n",
    "    * positional_embedding (english) → (None, None, 256) | 3,845,120 params\n",
    "    * positional_embedding (spanish) → (None, None, 256) | 3,845,120 params\n",
    "    * Setiap token (baik input maupun target) diubah menjadi vektor embedding berdimensi 256, lalu ditambahkan informasi posisi agar model tahu urutan kata.\n",
    "    * Parameter = vocab_size * embedding_dim (di sini sekitar 15k * 256 = 3.8 juta).\n",
    "3. Transformer Encoder\n",
    "    * transformer_encoder → (None, None, 256) | 3,155,456 params\n",
    "    * Menerima input embedding dari bahasa Inggris, lalu memprosesnya lewat:\n",
    "        - Multi-Head Self-Attention\n",
    "        - Feedforward Network (dense projection)\n",
    "        - Residual + Normalisasi\n",
    "    * Outputnya representasi kontekstual bahasa Inggris.\n",
    "4. Transformer Decoder\n",
    "    * transformer_decoder → (None, None, 256) | 5,259,520 params\n",
    "    * Menerima:\n",
    "      - Target sequence embedding (bahasa Spanyol)\n",
    "      - Representasi encoder (bahasa Inggris)\n",
    "    * Prosesnya meliputi:\n",
    "      - Masked Self-Attention (hanya boleh lihat token sebelumnya di output).\n",
    "      - Cross-Attention (menghubungkan target ke representasi encoder).\n",
    "      - Feedforward Network.\n",
    "    * Outputnya representasi token target yang sudah “tahu konteks input + output sebelumnya”.\n",
    "5. Dropout\n",
    "    * dropout_3 → (None, None, 256)\\\n",
    "      Lapisan regularisasi untuk mencegah overfitting.\n",
    "6. Dense (Output Layer)\n",
    "    * dense_4 (Dense) → (None, None, 15000) | 3,855,000 params\n",
    "    * Layer linear yang memetakan dimensi 256 ke ukuran kosakata target (15k).\n",
    "    * Setiap token target diproyeksikan ke distribusi probabilitas kata lewat softmax.\n",
    "7. Parameter Total\n",
    "    * Total params = ~20 juta (76 MB)\n",
    "    * Semua trainable (tidak ada parameter beku).\n",
    "8. Inti Arsitektur\n",
    "   * Model ini adalah seq2seq Transformer untuk machine translation (English → Spanish) dengan:\n",
    "     - Embedding dimensi 256\n",
    "     - Encoder + Decoder stack\n",
    "     - Ukuran kosakata target 15k\n",
    "     - Total 20 juta parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "0gXDlsdHSJsA",
    "outputId": "954dd941-caea-4f35-ece5-53e84775238b"
   },
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import plot_model\n",
    "#from IPython.display import Image\n",
    "\n",
    "#plot_model(transformer, to_file='transformer.png', show_shapes=True)\n",
    "#Image(filename='transformer.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWZOsUxrx-9b"
   },
   "source": [
    "Setting lebih ringan (cepat latihan) akan diberi nama vanguard dan setting yang awal diberi nama transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bangun Model Vanguard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menentukan konfigurasi awal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "dense_dim = 512\n",
    "num_heads = 4\n",
    "sequence_length = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mendefinisikan input untuk bahasa sumber (English) dalam bentuk urutan token ID, dengan panjang dinamis (None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representasi token English yang mempertimbangkan urutan/posisi kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representasi konteks penuh untuk seluruh kalimat input (English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input berupa token ID untuk bahasa target (Spanish), digunakan saat training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "m5_AHHrbyFhh"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanguard = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ english             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spanish             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,922,560</span> │ english[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,922,560</span> │ spanish[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │ positional_embed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">660,096</span> │ positional_embed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_deco… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,935,000</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ english             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spanish             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,922,560\u001b[0m │ english[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,922,560\u001b[0m │ spanish[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m396,032\u001b[0m │ positional_embed… │\n",
       "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m660,096\u001b[0m │ positional_embed… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ transformer_deco… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m1,935,000\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m15000\u001b[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,836,248</span> (26.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,836,248\u001b[0m (26.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,836,248</span> (26.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,836,248\u001b[0m (26.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vanguard.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretasi Model\n",
    "1. Input Layer\n",
    "    * english (InputLayer): menerima input berupa token urutan bahasa Inggris.\n",
    "    * spanish (InputLayer): menerima input berupa token urutan bahasa Spanyol.\n",
    "    * Keduanya berbentuk (None, None) → artinya panjang sequence fleksibel, batch size juga fleksibel.\n",
    "2. Positional Embedding\n",
    "    * english → positional_embedding:\n",
    "      - Token embedding + positional embedding, menghasilkan representasi dimensi 128.\n",
    "      - Jumlah parameter: 1,922,560\n",
    "      - → ini berasal dari ukuran kosakata (input_dim) × output_dim (128).\n",
    "    * spanish → positional_embedding_1:\n",
    "      - Sama dengan di atas, untuk bahasa Spanyol.\n",
    "      - Parameternya juga 1,922,560.\n",
    "      - Dua embedding ini memastikan bahwa baik token Inggris maupun Spanyol punya representasi vektor + informasi posisi dalam sequence.\n",
    "3. Transformer Encoder\n",
    "    * Input: embedding dari bahasa Inggris.\n",
    "    * Proses: self-attention + feedforward network + residual + layer norm.\n",
    "    * Output: representasi kontekstual Inggris dalam dimensi 128.\n",
    "    * Parameter: 396,032 (cukup ringan karena embed_dim hanya 128 dan jumlah head terbatas).\n",
    "4. Transformer Decoder\n",
    "    * Input utama: embedding Spanyol.\n",
    "    * Input tambahan: hasil dari encoder (bahasa Inggris).\n",
    "    * Proses: masked self-attention untuk decoder, cross-attention ke encoder output, lalu feedforward network.\n",
    "    * Output: representasi bahasa Spanyol yang sudah terhubung dengan konteks bahasa Inggris.\n",
    "    * Parameter: 660,096 (lebih besar daripada encoder, karena ada cross-attention tambahan).\n",
    "5. Dropout\n",
    "    * dropout_7: menjaga generalisasi, mencegah overfitting.\n",
    "6. Dense Output Layer\n",
    "    * dense_9 (Dense, 15000 units):\n",
    "    * Output berupa distribusi probabilitas ke 15.000 kata (asumsi vocab size target = 15k).\n",
    "    * Param: 1,935,000 (128 × 15000 + bias).\n",
    "7. Total Parameter\n",
    "    * 6,836,248 (~6.8M params)\n",
    "    * Sangat ringan dibanding model Transformer standar (misalnya original Transformer bisa ratusan juta parameter).\n",
    "    * Masuk akal disebut lebih “vanguard” karena compact, mudah dilatih dengan resource terbatas.\n",
    "8. Interpretasi Ringkas\n",
    "    * Model vanguard ini adalah seq2seq Transformer untuk terjemahan Inggris → Spanyol.\n",
    "    * Bagian encoder memahami kalimat Inggris.\n",
    "    * Bagian decoder menghasilkan kalimat Spanyol dengan memperhatikan hasil encoder.\n",
    "    * Ukuran embedding yang kecil (128) dan vocab terbatas (15k) membuat model ini ringan, dengan hanya 6.8 juta parameter.\n",
    "    * Dengan ini, model cocok untuk eksperimen akademik, portofolio, atau pelatihan di laptop tanpa GPU besar, sambil tetap mempertahankan arsitektur khas Transformer (encoder–decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "guado_pXSMAc",
    "outputId": "87b973d1-90ea-4e4f-8d90-f40b6106c33a"
   },
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import plot_model\n",
    "#plot_model(vanguard, to_file='vanguard.png', show_shapes=True)\n",
    "#from IPython.display import Image\n",
    "#Image(\"vanguard.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx45csyNo6sc"
   },
   "source": [
    "## Compile dan Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlV-JNHuYBWM",
    "outputId": "c31b233a-23b1-4ff1-a280-c6ab32350dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7119 - loss: 2.2096   \n",
      "Epoch 1: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1919s\u001b[0m 1s/step - accuracy: 0.7410 - loss: 1.7738 - val_accuracy: 0.8032 - val_loss: 1.2309\n",
      "Epoch 2/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8128 - loss: 1.1956   \n",
      "Epoch 2: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1915s\u001b[0m 1s/step - accuracy: 0.8258 - loss: 1.1047 - val_accuracy: 0.8527 - val_loss: 0.8826\n",
      "Epoch 3/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8513 - loss: 0.9121   \n",
      "Epoch 3: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1909s\u001b[0m 1s/step - accuracy: 0.8558 - loss: 0.8783 - val_accuracy: 0.8664 - val_loss: 0.7800\n",
      "Epoch 4/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8673 - loss: 0.7910   \n",
      "Epoch 4: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1915s\u001b[0m 1s/step - accuracy: 0.8700 - loss: 0.7721 - val_accuracy: 0.8732 - val_loss: 0.7397\n",
      "Epoch 5/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8771 - loss: 0.7197   \n",
      "Epoch 5: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1919s\u001b[0m 1s/step - accuracy: 0.8791 - loss: 0.7063 - val_accuracy: 0.8776 - val_loss: 0.7145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2535baf4830>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"language_translation_checkpoint.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    "    monitor=\"val_accuracy\"\n",
    ")\n",
    "\n",
    "transformer.fit(train_ds, epochs=5, validation_data=val_ds, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8mHnyfmyJaR"
   },
   "source": [
    "#### Vanguard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMsWkXCOyK2o",
    "outputId": "2ef11882-7ebd-4b17-f626-5a67591c0968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - accuracy: 0.7038 - loss: 2.5846  \n",
      "Epoch 1: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 475ms/step - accuracy: 0.7359 - loss: 1.9178 - val_accuracy: 0.7923 - val_loss: 1.3261\n",
      "Epoch 2/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.7992 - loss: 1.3207  \n",
      "Epoch 2: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 472ms/step - accuracy: 0.8105 - loss: 1.2402 - val_accuracy: 0.8381 - val_loss: 1.0146\n",
      "Epoch 3/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - accuracy: 0.8354 - loss: 1.0641  \n",
      "Epoch 3: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 473ms/step - accuracy: 0.8403 - loss: 1.0310 - val_accuracy: 0.8561 - val_loss: 0.8999\n",
      "Epoch 4/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.8521 - loss: 0.9484  \n",
      "Epoch 4: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 473ms/step - accuracy: 0.8546 - loss: 0.9320 - val_accuracy: 0.8611 - val_loss: 0.8575\n",
      "Epoch 5/5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.8615 - loss: 0.8867  \n",
      "Epoch 5: saving model to language_translation_checkpoint.weights.h5\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 473ms/step - accuracy: 0.8631 - loss: 0.8764 - val_accuracy: 0.8666 - val_loss: 0.8336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25329175e50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanguard.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"language_translation_checkpoint.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    "    monitor=\"val_accuracy\"\n",
    ")\n",
    "\n",
    "vanguard.fit(train_ds, epochs=5, validation_data=val_ds, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perbandingan Performa Pelatihan\n",
    "1. Awal Training (Epoch 1)\n",
    "    * Transformer: train acc 74%, val acc 80% (cukup tinggi sejak awal → cepat adaptasi).\n",
    "    * Vanguard: train acc 73%, val acc 79% (sedikit lebih rendah, tapi tetap bagus).\n",
    "2. Perkembangan Selama 5 Epoch\n",
    "    * Transformer: naik konsisten hingga val acc 87,7% dengan val loss 0.71.\n",
    "    * Vanguard: juga naik stabil tapi sedikit di bawah, val acc 86,6% dengan val loss 0.83.\n",
    "    * Transformer unggul sekitar +1% akurasi validasi dan lebih rendah val loss → lebih baik generalisasi.\n",
    "3. Kecepatan Training\n",
    "   * Transformer: 1900 detik/epoch (32 menit).\n",
    "   * Vanguard: 616 detik/epoch (10 menit).\n",
    "   * Vanguard 3x lebih cepat, tapi dengan trade-off akurasi sedikit lebih rendah.\n",
    "#### Interpretasi\n",
    "* Transformer memang lebih berat (kompleksitas self-attention, embedding lebih dalam, dll.), tapi memberi hasil lebih akurat dan stabil.\n",
    "* Vanguard lebih ringan dan cepat dilatih, cocok jika sumber daya terbatas, namun ada kompromi pada akurasi akhir.\n",
    "* Gap akurasi tidak terlalu besar (87,7% vs 86,6%), sehingga pilihan tergantung konteks penggunaan:\n",
    "    - Jika akurasi absolut sangat penting (misalnya machine translation production level) → pilih Transformer.\n",
    "    - Jika waktu dan resource jadi batasan (misalnya untuk prototyping, edge device, atau iterasi cepat) → Vanguard lebih efisien.\n",
    "* Hasil ini menunjukkan keunggulan Transformer bukan hanya sebagai model kuat untuk bahasa alami, tapi juga memberikan baseline yang sangat solid.\n",
    "* Vanguard sebagai varian yang lebih ringan bisa diposisikan sebagai alternatif trade-off antara akurasi dan kecepatan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2-E06uKo9ff"
   },
   "source": [
    "## Evaluate BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trM8O_CIZovW",
    "outputId": "6e16f33a-e2a1-46da-e71a-4c67e5a441e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Take it easy!\n",
      "Prediction: toma fácil end                 \n",
      "Target: [['¡Relajate!']]\n",
      "BLEU: 0.0000\n",
      "\n",
      "Input: Will he succeed or fail?\n",
      "Prediction: va a triunfar o no end              \n",
      "Target: [['¿Él', 'triunfará', 'o', 'fracasará?']]\n",
      "BLEU: 0.0000\n",
      "\n",
      "Input: I resign.\n",
      "Prediction: yo [UNK] end                 \n",
      "Target: [['Dimito.']]\n",
      "BLEU: 0.0000\n",
      "\n",
      "Input: He had more than enough money.\n",
      "Prediction: Él tuvo más de dinero suficiente dinero end            \n",
      "Target: [['Él', 'tenía', 'más', 'que', 'suficiente', 'dinero.']]\n",
      "BLEU: 0.0000\n",
      "\n",
      "Input: I was trying not to look.\n",
      "Prediction: no estaba intentando mirar end               \n",
      "Target: [['Trataba', 'de', 'no', 'mirar.']]\n",
      "BLEU: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Asumsikan test_pairs = [(eng, spa), ...]\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(sequence_length):\n",
    "        tokenized_target = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer.predict({\"english\": tokenized_input, \"spanish\": tokenized_target}, verbose=0)\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = target_vectorization.get_vocabulary()[sampled_token_index]\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "    return decoded_sentence.replace(\"[start] \", \"\")\n",
    "\n",
    "# BLEU Evaluation\n",
    "for _ in range(5):\n",
    "    input_text, target_text = random.choice(test_pairs)\n",
    "    prediction = decode_sequence(input_text)\n",
    "    reference = [target_text.replace(\"[start] \", \"\").replace(\" [end]\", \"\").split()]\n",
    "    candidate = prediction.split()\n",
    "    bleu_score = sentence_bleu(reference, candidate)\n",
    "    print(f\"Input: {input_text}\\nPrediction: {prediction}\\nTarget: {reference}\\nBLEU: {bleu_score:.4f}\\n\")\n",
    "\n",
    "### Save Final Model\n",
    "transformer.save(\"transformer_translation_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lSAj01_yNE6"
   },
   "source": [
    "#### Vanguard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBqC-zkMyOI6",
    "outputId": "90c8233e-8c01-4dc6-d65f-97d5f9289b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I hope we meet again someday soon.\n",
      "Prediction: espero que nos [UNK] otra vez end             \n",
      "Target: [['Espero', 'que', 'algún', 'día', 'pronto', 'nos', 'volvamos', 'a', 'ver.']]\n",
      "BLEU: 0.0000\n",
      "\n",
      "Input: Please put a lot of cream in my coffee.\n",
      "Prediction: por favor [UNK] mucho en el café end            \n",
      "Target: [['Ponele', 'mucha', 'crema', 'a', 'mi', 'café,', 'por', 'favor.']]\n",
      "BLEU: 0.0000\n",
      "\n",
      "Input: Come on, try again.\n",
      "Prediction: ven a intentar end                \n",
      "Target: [['Vamos,', 'inténtalo', 'otra', 'vez.']]\n",
      "BLEU: 0.0000\n",
      "\n",
      "Input: All human beings are mortal.\n",
      "Prediction: todas las [UNK] son [UNK] end              \n",
      "Target: [['Todos', 'los', 'humanos', 'son', 'mortales.']]\n",
      "BLEU: 0.0000\n",
      "\n",
      "Input: Can I have some of these?\n",
      "Prediction: puedo tener algo de estas end              \n",
      "Target: [['¿Puede', 'darme', 'algunos', 'de', 'éstos?']]\n",
      "BLEU: 0.0000\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTarget: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBLEU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m### Save Final Model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m vanguard\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvanguard_translation_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\zipfile\\__init__.py:1949\u001b[0m, in \u001b[0;36mZipFile.writestr\u001b[1;34m(self, zinfo_or_arcname, data, compress_type, compresslevel)\u001b[0m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(zinfo, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dest:\n\u001b[1;32m-> 1949\u001b[0m         dest\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\zipfile\\__init__.py:1252\u001b[0m, in \u001b[0;36m_ZipWriteFile.write\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compressor\u001b[38;5;241m.\u001b[39mcompress(data)\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nbytes\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# Assume test_pairs = [(eng, spa), ...]\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(sequence_length):\n",
    "        tokenized_target = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = vanguard.predict({\"english\": tokenized_input, \"spanish\": tokenized_target}, verbose=0)\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = target_vectorization.get_vocabulary()[sampled_token_index]\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "    return decoded_sentence.replace(\"[start] \", \"\")\n",
    "\n",
    "# BLEU Evaluation\n",
    "for _ in range(5):\n",
    "    input_text, target_text = random.choice(test_pairs)\n",
    "    prediction = decode_sequence(input_text)\n",
    "    reference = [target_text.replace(\"[start] \", \"\").replace(\" [end]\", \"\").split()]\n",
    "    candidate = prediction.split()\n",
    "    bleu_score = sentence_bleu(reference, candidate)\n",
    "    print(f\"Input: {input_text}\\nPrediction: {prediction}\\nTarget: {reference}\\nBLEU: {bleu_score:.4f}\\n\")\n",
    "\n",
    "### Save Final Model\n",
    "vanguard.save(\"vanguard_translation_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZwysUrQpBA0"
   },
   "source": [
    "### Menyimpan arsitektur model dalam file json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "SYqvQ-BXm8wd"
   },
   "outputs": [],
   "source": [
    "model_json = transformer.to_json()\n",
    "with open(\"translator.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV78i6xYnCyg"
   },
   "source": [
    "**Menerjemahkan kalimat baru dengan model Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "3atnDjYMnCgy"
   },
   "outputs": [],
   "source": [
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIsz7TWOyPOS"
   },
   "source": [
    "**Menerjemahkan kalimat baru dengan model vanguard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "eaeM28gVyQkC"
   },
   "outputs": [],
   "source": [
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5h5iutFnGO7"
   },
   "source": [
    "### Output Testing and Decoding the output sequence with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "nOcRWT4em95P"
   },
   "outputs": [],
   "source": [
    "def transformer_decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    transformer_decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [transformer_decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        transformer_decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return transformer_decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Testing and Decoding the output sequence with vanguard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanguard_decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    vanguard_decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [vanguard_decoded_sentence])[:, :-1]\n",
    "        predictions = vanguard(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        vanguard_decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return vanguard_decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPBhqPZhnLBK"
   },
   "source": [
    "### Transformer translating output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1SlMTGunHxH",
    "outputId": "4335720d-7387-4271-85ec-c353320092e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Does prison reform criminals?\n",
      "[start] [UNK] la cárcel end                \n",
      "-\n",
      "This juice would be even better with two ice cubes.\n",
      "[start] este jugo sería mejor que dos con dos hielo end          \n",
      "-\n",
      "Show me your passport, please.\n",
      "[start] muéstrame su pasaporte por favor end              \n",
      "-\n",
      "You have three seconds to make your choice.\n",
      "[start] tienes tres semanas end                \n",
      "-\n",
      "What he says is true.\n",
      "[start] lo que dice es cierto end              \n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(transformer_decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATLqYk6VySob"
   },
   "source": [
    "### Vanguard translating output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLyavKBmyTh1",
    "outputId": "30ce113b-c712-4723-ace4-166bab0abaec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "I can't decide which car to buy.\n",
      "[start] no puedo decidir qué auto end              \n",
      "-\n",
      "You're quite smart.\n",
      "[start] eres bastante inteligente end                \n",
      "-\n",
      "Tom is seeking a job.\n",
      "[start] tom está buscando trabajo end               \n",
      "-\n",
      "She was heard to criticize the manager.\n",
      "[start] ella estaba oído [UNK] a el [UNK] end            \n",
      "-\n",
      "Tom is extremely busy today.\n",
      "[start] tom está muy ocupado hoy end              \n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(vanguard_decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGgfTe4vnOBY"
   },
   "source": [
    "## Evaluation using the BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer BLEU Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0ZkyI6KnJBQ",
    "outputId": "e347cd61-cfcf-46db-a86d-ed7adcee8300",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fui a la tienda a comprar algo de comprar un [UNK] y [UNK] end       [start] fui a la tienda a comprar champú y dentífrico. [end]\n",
      "Transformer Score:0.2647058823529412\n",
      "dónde está un hospital end                [start] ¿dónde hay un hospital? [end]\n",
      "Transformer Score:0.34146341463414637\n",
      "es hora de ir a la escuela end             [start] es hora de ir al colegio. [end]\n",
      "Transformer Score:0.2857142857142857\n",
      "no trabajo el domingo end                [start] no trabajo el domingo. [end]\n",
      "Transformer Score:0.3499999999999999\n",
      "queremos información end                  [start] queremos información. [end]\n",
      "Transformer Score:0.3658536585365854\n",
      "tom debería estar bien el lunes end              [start] tom debería estar bien para el lunes. [end]\n",
      "Transformer Score:0.3125\n",
      "te [UNK] este formulario por favor end              [start] ¿podría cumplimentar este formulario, por favor? [end]\n",
      "Transformer Score:0.35294117647058826\n",
      "todos los gato se ama a mi gato end            [start] todo el mundo quiere a mi gato. [end]\n",
      "Transformer Score:0.2608695652173913\n",
      "esa corbata realmente se está realmente end              [start] esa corbata te queda bien. [end]\n",
      "Transformer Score:0.19642857142857142\n",
      "no tengo un lápiz end                [start] no tengo lápiz. [end]\n",
      "Transformer Score:0.3333333333333333\n",
      "es difícil ayudar a ayudar a la ayuda no le quieren tu ayuda end       [start] es difícil ayudar a la gente que no quiere ayuda. [end]\n",
      "Transformer Score:0.24285714285714285\n",
      "Él estaba en el auto end               [start] él durmió en el coche. [end]\n",
      "Transformer Score:0.2631578947368421\n",
      "tom fue [UNK] en un [UNK] en una televisión end           [start] tom estaba acurrucado en un sillón viendo tv. [end]\n",
      "Transformer Score:0.2807017543859649\n",
      "ellos se dice que la vida es tan pequeño end           [start] ellos a menudo dicen que la vida es corta. [end]\n",
      "Transformer Score:0.25925925925925924\n",
      "sé que eres un profesor end               [start] sé que tú eres profesor. [end]\n",
      "Transformer Score:0.2926829268292683\n",
      "[UNK] el plan hasta las diez end              [start] ¿pretendes trabajar hasta las diez? [end]\n",
      "Transformer Score:0.31111111111111106\n",
      "no le digas a nadie más end              [start] pero no le digas a nadie más. [end]\n",
      "Transformer Score:0.3\n",
      "siempre te he [UNK] end                [start] siempre te amé. [end]\n",
      "Transformer Score:0.3157894736842105\n",
      "no pude [UNK] mi [UNK] end               [start] no pude contener mi ira. [end]\n",
      "Transformer Score:0.275\n",
      "está muy enfadado end                 [start] está realmente enfadado. [end]\n",
      "Transformer Score:0.2972972972972973\n",
      "\n",
      "Transformer BLEU score : 5.9/20\n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "test_spa_texts = [pair[1] for pair in test_pairs]\n",
    "transformer_score = 0\n",
    "transformer_bleu  = 0\n",
    "for i in range(20):\n",
    "    candidate = decode_sequence(test_eng_texts[i])\n",
    "    reference = test_spa_texts[i].lower()\n",
    "    print(candidate,reference)\n",
    "    transformer_score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "    transformer_bleu+=transformer_score\n",
    "    print(f\"Transformer Score:{transformer_score}\")\n",
    "print(f\"\\nTransformer BLEU score : {round(transformer_bleu,2)}/20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "enU_rcSFnPVG",
    "outputId": "e55ec9a9-b7c7-434f-adf8-3433f788f6c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer BLEU score : 5.9/20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Transformer BLEU score : {round(transformer_bleu,2)}/20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozCcSkc8yVfT"
   },
   "source": [
    "### Vanguard BLEU Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6GPXSWKyWkL",
    "outputId": "de1ca1fb-a4fe-429f-cfd7-a5452029412c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fui a la tienda a comprar algo de comprar un [UNK] y [UNK] end       [start] fui a la tienda a comprar champú y dentífrico. [end]\n",
      "Vanguard Score:0.2647058823529412\n",
      "dónde está un hospital end                [start] ¿dónde hay un hospital? [end]\n",
      "Vanguard Score:0.34146341463414637\n",
      "es hora de ir a la escuela end             [start] es hora de ir al colegio. [end]\n",
      "Vanguard Score:0.2857142857142857\n",
      "no trabajo el domingo end                [start] no trabajo el domingo. [end]\n",
      "Vanguard Score:0.3499999999999999\n",
      "queremos información end                  [start] queremos información. [end]\n",
      "Vanguard Score:0.3658536585365854\n",
      "tom debería estar bien el lunes end              [start] tom debería estar bien para el lunes. [end]\n",
      "Vanguard Score:0.3125\n",
      "te [UNK] este formulario por favor end              [start] ¿podría cumplimentar este formulario, por favor? [end]\n",
      "Vanguard Score:0.35294117647058826\n",
      "todos los gato se ama a mi gato end            [start] todo el mundo quiere a mi gato. [end]\n",
      "Vanguard Score:0.2608695652173913\n",
      "esa corbata realmente se está realmente end              [start] esa corbata te queda bien. [end]\n",
      "Vanguard Score:0.19642857142857142\n",
      "no tengo un lápiz end                [start] no tengo lápiz. [end]\n",
      "Vanguard Score:0.3333333333333333\n",
      "es difícil ayudar a ayudar a la ayuda no le quieren tu ayuda end       [start] es difícil ayudar a la gente que no quiere ayuda. [end]\n",
      "Vanguard Score:0.24285714285714285\n",
      "Él estaba en el auto end               [start] él durmió en el coche. [end]\n",
      "Vanguard Score:0.2631578947368421\n",
      "tom fue [UNK] en un [UNK] en una televisión end           [start] tom estaba acurrucado en un sillón viendo tv. [end]\n",
      "Vanguard Score:0.2807017543859649\n",
      "ellos se dice que la vida es tan pequeño end           [start] ellos a menudo dicen que la vida es corta. [end]\n",
      "Vanguard Score:0.25925925925925924\n",
      "sé que eres un profesor end               [start] sé que tú eres profesor. [end]\n",
      "Vanguard Score:0.2926829268292683\n",
      "[UNK] el plan hasta las diez end              [start] ¿pretendes trabajar hasta las diez? [end]\n",
      "Vanguard Score:0.31111111111111106\n",
      "no le digas a nadie más end              [start] pero no le digas a nadie más. [end]\n",
      "Vanguard Score:0.3\n",
      "siempre te he [UNK] end                [start] siempre te amé. [end]\n",
      "Vanguard Score:0.3157894736842105\n",
      "no pude [UNK] mi [UNK] end               [start] no pude contener mi ira. [end]\n",
      "Vanguard Score:0.275\n",
      "está muy enfadado end                 [start] está realmente enfadado. [end]\n",
      "Vanguard Score:0.2972972972972973\n",
      "Vanguard BLEU score : 5.9/20\n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "test_spa_texts = [pair[1] for pair in test_pairs]\n",
    "vanguard_score = 0\n",
    "vanguard_bleu  = 0\n",
    "for i in range(20):\n",
    "    candidate = decode_sequence(test_eng_texts[i])\n",
    "    reference = test_spa_texts[i].lower()\n",
    "    print(candidate,reference)\n",
    "    vanguard_score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "    vanguard_bleu+=vanguard_score\n",
    "    print(f\"Vanguard Score:{vanguard_score}\")\n",
    "print(f\"Vanguard BLEU score : {round(vanguard_bleu,2)}/20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBnMmCeryXq1",
    "outputId": "1f1ed14a-29ba-4bde-f802-643a98b0a589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanguard BLEU score : 5.9/20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vanguard BLEU score : {round(vanguard_bleu,2)}/20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O2MEOEv1nXTq",
    "outputId": "754c57cb-5faf-4004-c418-e7da44c326c7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH   : She denied having taken part in the scheme.\n",
      "PREDICTED : [start] ella negó haber tomado parte en el plan end           \n",
      "TARGET    : Ella negó haber tomado parte en el plan.\n",
      "BLEU SCORE: 0.5170\n",
      "\n",
      "ENGLISH   : His memory never ceases to astonish me.\n",
      "PREDICTED : [start] su memoria no me [UNK] a [UNK] end            \n",
      "TARGET    : Su memoria me sorprende.\n",
      "BLEU SCORE: 0.0000\n",
      "\n",
      "ENGLISH   : No, I'm not a teacher. I'm only a student.\n",
      "PREDICTED : [start] no soy profesor solo un profesor solo estudiante end           \n",
      "TARGET    : No, no soy maestro. Soy solo un estudiante.\n",
      "BLEU SCORE: 0.0000\n",
      "\n",
      "ENGLISH   : I'll be glad to.\n",
      "PREDICTED : [start] me haré feliz end                \n",
      "TARGET    : Será un placer.\n",
      "BLEU SCORE: 0.0000\n",
      "\n",
      "ENGLISH   : He was wrong in thinking that she'd come to see him.\n",
      "PREDICTED : [start] Él estaba equivocado en ese día en ver que lo ver end        \n",
      "TARGET    : Él se equivocaba al pensar que ella vendría a verle.\n",
      "BLEU SCORE: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Ambil sampel acak dari data validasi\n",
    "for _ in range(5):\n",
    "    input_text, target_text = random.choice(val_pairs)\n",
    "    prediction = transformer_decode_sequence(input_text)\n",
    "\n",
    "    # Format referensi dan prediksi\n",
    "    reference = [target_text.replace(\"[start] \", \"\").replace(\" [end]\", \"\").split()]\n",
    "    candidate = prediction.split()\n",
    "\n",
    "    transformer_bleu = sentence_bleu(reference, candidate)\n",
    "\n",
    "    print(f\"ENGLISH   : {input_text}\")\n",
    "    print(f\"PREDICTED : {prediction}\")\n",
    "    print(f\"TARGET    : {' '.join(reference[0])}\")\n",
    "    print(f\"BLEU SCORE: {transformer_bleu:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HI80AqunyZO8"
   },
   "source": [
    "#### Translation with vanguard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ivp5gnLncjx",
    "outputId": "76ddf643-1efb-44ba-d860-f315c7a2e739",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH   : Tom knows the man Mary came with.\n",
      "PREDICTED : [start] tom sabe el hombre que vino con mary end           \n",
      "TARGET    : Tom conoce al hombre con el que vino Mary.\n",
      "BLEU SCORE: 0.0000\n",
      "\n",
      "ENGLISH   : Come on in. The water's nice.\n",
      "PREDICTED : [start] ven a el buen agua end              \n",
      "TARGET    : Métete. El agua está rica.\n",
      "BLEU SCORE: 0.0000\n",
      "\n",
      "ENGLISH   : I don't know my father's annual income.\n",
      "PREDICTED : [start] no sé mi padre [UNK] [UNK] end             \n",
      "TARGET    : No conozco los ingresos anuales de mi padre.\n",
      "BLEU SCORE: 0.0000\n",
      "\n",
      "ENGLISH   : Get on your knees.\n",
      "PREDICTED : [start] [UNK] las [UNK] end                \n",
      "TARGET    : Arrodíllate.\n",
      "BLEU SCORE: 0.0000\n",
      "\n",
      "ENGLISH   : I have two dogs, three cats, and six chickens.\n",
      "PREDICTED : [start] tengo dos perros tres gatos y seis [UNK] end           \n",
      "TARGET    : Tengo dos perros, tres gatos y seis gallinas.\n",
      "BLEU SCORE: 0.2778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Ambil sampel acak dari data validasi\n",
    "for _ in range(5):\n",
    "    input_text, target_text = random.choice(val_pairs)\n",
    "    prediction = vanguard_decode_sequence(input_text)\n",
    "\n",
    "    # Format referensi dan prediksi\n",
    "    reference = [target_text.replace(\"[start] \", \"\").replace(\" [end]\", \"\").split()]\n",
    "    candidate = prediction.split()\n",
    "\n",
    "    vanguard_bleu = sentence_bleu(reference, candidate)\n",
    "\n",
    "    print(f\"ENGLISH   : {input_text}\")\n",
    "    print(f\"PREDICTED : {prediction}\")\n",
    "    print(f\"TARGET    : {' '.join(reference[0])}\")\n",
    "    print(f\"BLEU SCORE: {vanguard_bleu:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Transformer BLEU score: 0.1030\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Siapkan list untuk references dan candidates\n",
    "references = []\n",
    "candidates = []\n",
    "\n",
    "# Loop seluruh pasangan kalimat di test set\n",
    "for input_text, target_text in test_pairs:\n",
    "    prediction = transformer_decode_sequence(input_text)\n",
    "    \n",
    "    # Bersihkan target (hilangkan token start/end)\n",
    "    reference = target_text.replace(\"[start] \", \"\").replace(\" [end]\", \"\").split()\n",
    "    candidate = prediction.split()\n",
    "    \n",
    "    # Tambahkan ke list untuk corpus BLEU\n",
    "    references.append([reference])   # penting: list of list\n",
    "    candidates.append(candidate)\n",
    "\n",
    "# Hitung corpus BLEU\n",
    "transformer_bleu_score = corpus_bleu(references, candidates)\n",
    "print(f\"Corpus Transformer BLEU score: {transformer_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Vanguard BLEU score: 0.0867\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Siapkan list untuk references dan candidates\n",
    "references = []\n",
    "candidates = []\n",
    "\n",
    "# Loop seluruh pasangan kalimat di test set\n",
    "for input_text, target_text in test_pairs:\n",
    "    prediction = vanguard_decode_sequence(input_text)\n",
    "    \n",
    "    # Bersihkan target (hilangkan token start/end)\n",
    "    reference = target_text.replace(\"[start] \", \"\").replace(\" [end]\", \"\").split()\n",
    "    candidate = prediction.split()\n",
    "    \n",
    "    # Tambahkan ke list untuk corpus BLEU\n",
    "    references.append([reference])   # penting: list of list\n",
    "    candidates.append(candidate)\n",
    "\n",
    "# Hitung corpus BLEU\n",
    "vanguard_bleu_score = corpus_bleu(references, candidates)\n",
    "print(f\"Corpus Vanguard BLEU score: {vanguard_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqfXVURfygdb",
    "outputId": "863bc62e-f2f8-4490-bdc3-f5851f46d5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 269ms/step - accuracy: 0.8776 - loss: 0.7145 \n",
      "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 156ms/step - accuracy: 0.8666 - loss: 0.8336\n",
      "Transformer Evaluation: \n",
      "Validation Accuracy: 0.8776\n",
      "Validation Loss: 0.7145\n",
      "Transformer BLEU score : 0.0/20\n",
      "\n",
      "Vanguard Evaluation: \n",
      "Validation Accuracy: 0.8666\n",
      "Validation Loss: 0.8336\n",
      "Vanguard BLEU score : 0.28/20\n"
     ]
    }
   ],
   "source": [
    "transformer_loss, transformer_acc = transformer.evaluate(val_ds)\n",
    "vanguard_loss, vanguard_acc = vanguard.evaluate(val_ds)\n",
    "print(\"Transformer Evaluation: \")\n",
    "print(f\"Validation Accuracy: {transformer_acc:.4f}\")\n",
    "print(f\"Validation Loss: {transformer_loss:.4f}\")\n",
    "print(f\"Transformer BLEU score : {round(transformer_bleu_score,2)}/20\")\n",
    "print(\"\\nVanguard Evaluation: \")\n",
    "print(f\"Validation Accuracy: {vanguard_acc:.4f}\")\n",
    "print(f\"Validation Loss: {vanguard_loss:.4f}\")\n",
    "print(f\"Vanguard BLEU score : {round(vanguard_bleu,2)}/20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 274ms/step - accuracy: 0.8776 - loss: 0.7145 \n",
      "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 151ms/step - accuracy: 0.8666 - loss: 0.8336\n",
      "Transformer Evaluation: \n",
      "Validation Accuracy: 0.8776\n",
      "Validation Loss: 0.7145\n",
      "Corpus Transformer BLEU score : 0.1030\n",
      "\n",
      "Vanguard Evaluation: \n",
      "Validation Accuracy: 0.8666\n",
      "Validation Loss: 0.8336\n",
      "Corpus Vanguard BLEU score : 0.0867\n"
     ]
    }
   ],
   "source": [
    "transformer_loss, transformer_acc = transformer.evaluate(val_ds)\n",
    "vanguard_loss, vanguard_acc = vanguard.evaluate(val_ds)\n",
    "print(\"Transformer Evaluation: \")\n",
    "print(f\"Validation Accuracy: {transformer_acc:.4f}\")\n",
    "print(f\"Validation Loss: {transformer_loss:.4f}\")\n",
    "print(f\"Corpus Transformer BLEU score : {transformer_bleu_score:.4f}\")\n",
    "print(\"\\nVanguard Evaluation: \")\n",
    "print(f\"Validation Accuracy: {vanguard_acc:.4f}\")\n",
    "print(f\"Validation Loss: {vanguard_loss:.4f}\")\n",
    "print(f\"Corpus Vanguard BLEU score : {vanguard_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretasi\n",
    "1. Akurasi dan Loss\n",
    "    * Transformer unggul dalam akurasi (87.76% vs 86.66%) dan loss lebih rendah (0.71 vs 0.83).\n",
    "    * Artinya, dari sisi prediksi token per token, Transformer lebih baik dalam menebak kata yang benar.\n",
    "2. Corpus BLEU\n",
    "    * Nilai BLEU keduanya masih rendah (sekitar 0.1).\n",
    "    * Ini umum terjadi ketika model belum benar-benar fasih membentuk kalimat yang sesuai dengan ground truth.\n",
    "    * Namun Transformer memiliki skor BLEU lebih tinggi (0.1030 vs 0.0867), menunjukkan kualitas urutan kata dan kesesuaian n-gram lebih baik dibanding Vanguard.\n",
    "3. Kesesuaian antar-metrik\n",
    "    * Akurasi tinggi + BLEU rendah → menunjukkan bahwa meskipun banyak kata yang diprediksi benar, urutan atau struktur kalimatnya sering tidak sesuai.\n",
    "    * Transformer unggul tipis di BLEU, artinya selain lebih tepat di level kata, ia juga sedikit lebih baik dalam menyusun kata menjadi kalimat yang mirip target.\n",
    "4. Kesimpulan\n",
    "    * Transformer konsisten lebih unggul dibanding Vanguard di semua metrik (akurasi, loss, dan corpus BLEU).\n",
    "    * Nilai BLEU yang rendah (sekitar 0.1) menandakan model masih jauh dari kualitas natural dalam menyusun kalimat, meskipun akurasi token sudah relatif tinggi.\n",
    "    * Ini memperlihatkan bahwa akurasi tidak cukup untuk menilai kualitas teks, dan corpus BLEU membantu menunjukkan kelemahan model dalam menghasilkan urutan kata yang koheren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu94sO_w3j-b"
   },
   "source": [
    "# Kesimpulan\n",
    "Hasil evaluasi menunjukkan bahwa Transformer memiliki keunggulan dalam akurasi dan loss, menandakan ketepatan prediksi token yang lebih baik dibanding Vanguard. Namun, nilai Corpus BLEU yang relatif rendah pada keduanya mengindikasikan bahwa susunan kalimat masih jauh dari ideal, walaupun prediksi kata per kata cukup tepat. Artinya, kualitas sintaksis dan semantik kalimat belum sepenuhnya terjaga. Kondisi ini menekankan bahwa akurasi dan BLEU tidak boleh dilihat terpisah, melainkan saling melengkapi, akurasi untuk menilai prediksi token, BLEU untuk menilai kelancaran kalimat. Dengan perbaikan strategi decoding (misalnya beam search) dan fine-tuning yang lebih matang, skor BLEU dapat ditingkatkan, sehingga Transformer semakin mendekati kemampuannya sebagai pondasi utama model bahasa generatif modern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
